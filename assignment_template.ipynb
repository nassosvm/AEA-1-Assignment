{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wellbeing and Development Index: A Supplementary Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Athanasios Vourtsis Murat\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Name|SNR|ANR|\n",
    "|----|---|----|\n",
    "|Athanasios Vourtsis Murat|2062843|u312241|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research question\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b350c3082e112a5b2f1f6db0481fa5bd",
     "grade": true,
     "grade_id": "cell-7a61a5e9e6991ff4",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Could there be a second-order index (composite index {Wellbeing and Human Development} of composite indexes of categories vital to the former {Education, Environment, Health, Infrastructure, Labor, Social Conditions}) that formulate in an as transparent as possible manner, the parameters that set the basis for societal cohesion, apart from the relatively easily accessible quantitively monetary measure of GDP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ab93f4d26eb3e0bcf88927ee71df6b63",
     "grade": true,
     "grade_id": "cell-3d71712c92143820",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "The latest societal, environmental, and humanitarian developments indicate that the use of GDP as the sole indicator of economic and social prosperity may be misleading and lead to uninformed and partially effective policymaking, being incompatible with the betterment of qualitative parameters that constitute the overall wellbeing. Based on the above premise, it would be of great value if one or more compounded indexes that encapsulate the different societal facets were in place, not by necessarily substituting, but complementing the strictly quantitative and general economic performance measures. Thus, the analysis of this assignment is focused on developing a complementary to the GDP Wellbeing and Development Index.\n",
    "\n",
    "Starting this venture of scientific inquiry, we shall distinguish the categories of most importance that are not directly assumed by the GDP measure. For example, it could be the case that the quality of life in a relatively higher income country is on average higher than a lower-income country. Nevertheless, there may still be some structural societal irregularities that obstruct the given society to reach its full potential from a perspective of cohesion and wellbeing. In other words, the total wealth produced from an economy does not equate necessarily with the state of societal integration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data and Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Concerning the Data and their Respective Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most crucial categories that has been disturbingly left out of the growth models and has been the subject of intense debate and radical proposals, is that of environmental sustainability and biodiversity. At this point, it is an indisputable fact that the ecosystems of the earth are heading to a tipping point of no return due to the footprint of the reckless production methods that humanity has been adopting and continues to pursue. The main variables used to characterize this category are the Emissions per Capita (1), Pesticide Use (2), and the Percentage of Electricity Produced from Renewable Energy Sources (3). Then categories as Education, Labor, and Infrastructure Conditions can prove of great importance. Respectively, the variables used for the identification of those categories are second and tertiary education enrollment rates (4),(5), mean years of education (6), youth (7), and general unemployment (8), yearly working hours (9), mobile cellular subscriptions per 100 people (10), internet use (11), and access to electricity (12).\n",
    "\n",
    "The last two main and most broad categories are those of Health and Social Conditions. The former pillar of an orderly societal functioning consists of the variables of air pollution deaths as a proportion of total deaths (13), the proportion of the population that presents alcohol and substance use disorders (14), life expectancy (15), the prevalence of HIV (16), hospital beds per 1000 people (17), the age-standardized prevalence of mental health disorders (18), the proportion of the population being overweight (19), the proportion of pregnant women facing anemia (20), the per capita protein supply (21), and the proportion of deaths caused by unsafe sanitation (22) and water sources (23). Then, for the latter one, variables and scores of corruption (24), political stability (25), rule of law (26), and the human rights conditions (27), the Gini index (28), Female-to-Male Ratio in the Labor Market (29), the income share of the bottom 10% (30), the share of deaths from homicide (31), deaths from disasters (32), and the share of deaths from suicide were used (33). \n",
    "\n",
    "\n",
    "(1): https://databank.worldbank.org/reports.aspx?source=2&series=EN.ATM.CO2E.PC&country=\n",
    "(2): https://ourworldindata.org/grapher/pesticide-use-tonnes?tab=table\n",
    "(3): https://ourworldindata.org/grapher/share-electricity-renewables?time=2019\n",
    "(4): https://databank.worldbank.org/reports.aspx?source=2&series=SE.SEC.ENRR&country=\n",
    "(5): https://databank.worldbank.org/reports.aspx?source=2&series=SE.TER.ENRR&country=\n",
    "(6): https://ourworldindata.org/grapher/mean-years-of-schooling-1?time=2017\n",
    "(7): https://databank.worldbank.org/reports.aspx?source=2&series=SL.UEM.1524.ZS&country=\n",
    "(8): https://data.worldbank.org/indicator/SL.UEM.TOTL.ZS\n",
    "(9): https://ourworldindata.org/grapher/annual-working-hours-per-worker?tab=chart&stackMode=absolute&time=1870..latest&region=World\n",
    "(10): https://databank.worldbank.org/reports.aspx?source=2&series=IT.CEL.SETS.P2&country=\n",
    "(11): https://ourworldindata.org/grapher/number-of-internet-users-by-country?time=2017\n",
    "(12): https://ourworldindata.org/grapher/share-of-the-population-with-access-to-electricity?region=World\n",
    "(13): https://ourworldindata.org/grapher/share-deaths-air-pollution?time=2017\n",
    "(14): https://ourworldindata.org/grapher/share-of-population-with-drug-use-disorders?time=2016\n",
    "(15): https://ourworldindata.org/grapher/life-expectancy?tab=table&time=1917..2015&region=World\n",
    "(16): https://databank.worldbank.org/reports.aspx?source=2&series=SH.DYN.AIDS.ZS&country=\n",
    "(17): https://databank.worldbank.org/reports.aspx?source=2&series=SH.MED.BEDS.ZS&country=\n",
    "(18): https://ourworldindata.org/grapher/share-with-mental-health-or-development-disorder?time=2016\n",
    "(19): https://databank.worldbank.org/reports.aspx?source=311&series=SH.STA.OWAD.ZS\n",
    "(20): https://ourworldindata.org/grapher/prevalence-of-anemia-in-pregnant-women?time=2016\n",
    "(21): https://ourworldindata.org/grapher/daily-per-capita-protein-supply?time=2017\n",
    "(22): https://ourworldindata.org/grapher/death-rate-from-unsafe-sanitation?time=2017\n",
    "(23): https://ourworldindata.org/grapher/death-rates-unsafe-water?time=2017\n",
    "(24), (25), (26): https://databank.worldbank.org/source/worldwide-governance-indicators/preview/on\n",
    "(27): https://ourworldindata.org/grapher/human-rights-scores\n",
    "(28): https://databank.worldbank.org/reports.aspx?source=2&series=SI.POV.GINI&country=\n",
    "(29): https://databank.worldbank.org/reports.aspx?source=2&series=SL.TLF.CACT.FM.ZS&country=\n",
    "(30): https://databank.worldbank.org/reports.aspx?source=2&series=SI.DST.FRST.10&country=\n",
    "(31): https://ourworldindata.org/grapher/share-of-deaths-homicides?time=2017\n",
    "(32): https://ourworldindata.org/grapher/share-deaths-from-natural-disasters?tab=table\n",
    "(33): https://ourworldindata.org/grapher/share-deaths-suicide?time=2017\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Manipulation Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "All of the time series data mentioned above were placed in different folders based on their characteristics commonly attributed to them in the bibliography and other frameworks. Thus, the very first and brief procedure, that of the differentiation of the variables to be used, was a manual one and led to the location and creation of six different data characterizations. As already mentioned above, those are Education, Environment, Health, Infrastructure, Labor, and Social Conditions (Path Example; Shortened: Variables\\\\Category Type {6}\\\\Time_Series_Data.csv {33}).\n",
    "\n",
    "\n",
    "At the beginning of the algorithm a for-loop is being employed to access the folders and then the data seperately located in the major \"Variable\" folder to collect and append their respective paths in a dedicated list. In turn, that list was used in another for-loop to import the datasets as dataframes and append them to a list. \n",
    "\n",
    "\n",
    "Then, for-loops were used to the above list of dataframes to extract the exact names of the variables included and the intersection of the countries included in all of the datasets (achieving consistency across the large number of variables), eventually appending them to their separate lists as strings. The procedure of \"distillation\", shortly on will prove useful, as the lists of countries and variables occuring could function as masks. \n",
    "\n",
    "\n",
    "To further improve the structure of the data, a mask of the years to be used in the final analysis was being developed and implemented. Also, all the data points across the different data frames were assigned an index of date formatting based on their year information. Eventually, the intersecting countries list was used to create a list of data frames with a for-loop of each country yearly data for each of the variables (i.e. List Dimension=1650 (: {33 Variables}x{50 Countries}), Data Frames Dimensions={16,5} (: rows=16 years, col=Country, Country Code, Year, Variable, Value). Then, the list was split into 50 different arrays that contained the yearly data of each country and every variable, to be used right after in a for-loop to interpolate and fill the missing values, and their respective column information. Finally, for this final list, a for-loop removed all of the redundant columns, achieving a perfect uniformity across the Data Frames, which would eventually help to construct a merged data frame containing all data points (i.e. 26400; 16 x 33 x 50). \n",
    "\n",
    "\n",
    "That merged, filtered, uniformed, and cleaned data frame, functions as a basis/starting point to the designing of all data frames to be used in the statistical analyses. For example, that of a list of a 16 data frames based on the number of years of the series, which contain cross-sectional data for the 50 countries and across 33 variables. That list practically consitutes a panel.\n",
    "\n",
    "\n",
    "It shall be noted that an additional technical step took place during the analysis, that of reverse scoring. Reverse scoring is a procedure that reconfigures the direction of variables that run in the opposite direction of the remaining ones, qualitatively. The number of deaths or the per capita emissions is qualitatively opposite to, for example, life expectancy and use of renewables. Alternatively, an increasing number of deaths from various causes is unanimously considered a negative occurrence, while an increase in life expectancy is an undoubtedly positive development. The simplest and most efficient method used to reverse the direction of a variable is by deducting from a value of a given observation the maximum value presented among all the observations of the variable. Thus, the maximum value becomes the lowest possible and the lowest becomes the highest in the dataset. Reverse scoring is a crucial and essential step, in the absence of which the statistical analyses are seriously flawed (e.g. negative values for the Cronbach's Coefficient)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "84c7d0d85d8fb5e0073f3053f9bedf5a",
     "grade": true,
     "grade_id": "cell-8cbf4938fbebbb6e",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "The main statistical method used is that of Factor Analysis, and supplementary, the statistical measure of Cronbach's Coefficient Alpha. \n",
    "The latter one is used to check on the internal consistency of a set of variables that embark on approximating a hypothesized and initially qualitative variable. The main components of its' formula are the number of the variables used to construct an individual score, and most importantly, the one with the mean correlations among the variables. The mean correlation is a parameter occurring from the sum of all correlation values among all variables, divided by the number of variable combinations (i.e. for a set of 5 variables, we would get the correlations of variable 1 with the variables 2 to 5, then the correlations of variable 2 with the variables 3 to 5, the correlation of variable 3 with the variables 4 and 5, and finally, the correlation of variable 4 with 5, which totals 2 x (# of variables) => 2 x 5 = 10). Eventually, a high correlation among the variables produces a high alpha coefficient and indicates the internal consistency of the given compounded construct. A satisfactory value of the coefficient may start at as low as the low 60s, and a more than satisfactory one could be over 90. However, an alpha coefficient that is too high (>>95) could indicate redundancy of one or a set of variables used. \n",
    "\n",
    "\n",
    "From the brief explanation of Cronbach's Coefficient Alpha, it is clear that the measure carries no information on the dimensionality of the compounded score and the total variance, and which variable constitutes it, and to what extent.\n",
    "\n",
    "\n",
    "What Factor Analysis is mainly good for, is the reduction of dimensions in cases of high dimensionality, by locating underlying variables (factors) based on the shared variances that the multiple variables used present. Then the number of factors is determined by the chosen criterion (e.g. Kaiser, Horn's Parallel Analysis, etc), which (based on the test number of factors) yields some values, or more precisely, eigenvalues. Usually, the number of factors that are eventually used have an eigenvalue of at least one.\n",
    "\n",
    "\n",
    "After determining the suggested number of factors based on the criterion used, we run a factor analysis that yields a matrix of n(Variables) x m(# Factors), where for each variable and factor, a value is assigned, or alternatively a loading. What that indicates is the extent that a given variable contributes to the variance of a given factor, and thus, the extent that it is related. The loading value could be either negative or positive, and it ranges from -1 to 1. High enough loadings for a given factor and a set of variables (both positive and negative; in this case, the item is subtracted from the score rather than added) constitute a compounded underlying variable. Furthermore, it could be the case that a factor eventually has no explanatory value based on the variables' loadings, regardless of having an eigenvalue equal to or higher than one in the first step of the analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preview of the answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "938eb25dc709a2252d7013dc1cd8176a",
     "grade": true,
     "grade_id": "cell-1b3df4385bba508d",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "The first-order Cronbach’s Coefficient measures used to check the inner validity of the individual indexes per year yielded the following results. For the first year, “education” has an alpha of 0.88, “environment” has an alpha of 0.5, “health” has an alpha of 0.6, “infrastructure” has an alpha of 0.8, “labor” alpha has an alpha of 0.47, and “social conditions” has an alpha of 0.72. Eventually, the manually constructed categories’ alphas, averaged over 16 years of available data, are 0.87, 0.42, 0.59, 0.82, 0.43, and 0.71, respectively. That means the categories of education, infrastructure and that of social conditions present an adequate to very good internal validity, while the health category presents a marginally efficient one. The remaining categories of environment and infrastructure have poor internal validity, and thus, they may not be appropriate for our analysis.\n",
    "\n",
    "\n",
    "The first-order factor analysis for a fixed number of three factors per manually determined category per year yielded the following results. For the first year, education’s loadings indicate two underlying variables. That result was expected as in this category just 3 variables can be found, while the two of them relate to enrollment rates (for secondary and tertiary education). Then, the environment’s loadings technically indicate 3 different factors, which equal the exact number of variables available in this category. Thus, each one of them will act as a factor of themselves, that is, staying unchanged in the analysis. Health’s loadings effectively indicate three different factors that include {Life Expectancy (-0.68), Prevalence of Overweight People (-0.6), Protein Supply (-0.56), Unsafe Sanitation (0.99) and Unsafe Water Sources (0.98)}, {Air Pollution (-0.69), Drug Abuse (0.34), Prevalence of HIV (-0.38), Pregnant Anemia (-0.68)}, {Hospital Beds per 1000 people (-0.54), Prevalence of Mental Health Issues (0.64)}. Infrastructure and Labor Loadings do not strongly differentiate any underlying factors (low number of variables; 3 for both categories), and finally, the category of the social conditions locate the factors {Corruption Control (0.82), Female-to-Male Labor (0.89), Prevalence of Homicides (0.63), Deaths by Disasters (0.9), Prevalence of Suicides (0.81), Human Rights Conditions (0.21)}, {Gini Index (0.9), Income Share of the Bottom 10% (-0.89)}, {Political Stability (-0.49), Rule of Law (-0.45)}. It should be noted that these results vary greatly across the years of the available data, producing a state of inconclusiveness. That state is being further substantiated, as the eigenvalues for each of the categories give results of no value (the number of factors proposed is almost 1:1 to the variables available for each category).  \n",
    "\n",
    "\n",
    "Also, when it comes to the factors of the variables with more than 3 variables, the variables’ loadings constituting each of the factors present as having the opposite direction of the compounded variable/variance of the factor. In our case, it seems like the loadings of the factors for each variable have unexpected values and constitute a construct that cannot be generalized to a certain quality. Alternatively, some factors based on the “participating” variables, assign negative loadings to variables that should be of the opposite direction qualitatively, and vice versa. The procedure of factor constructing, in this case, does not lead to results that make sense based on the frameworks that each of the variables, and possible effective combinations of them, come from, as there is a mix of conflicting constructs. Thus, at this first-order analysis, equal weights were assigned to each of the manually constructed categories.\n",
    "\n",
    "\n",
    "Then, the same procedure was repeated, and equivalent results were drawn. The “second-order” Cronbach’s Alpha, or the internal consistency of the index of interest (that of Wellbeing and Development), is high enough to be considered reliable (0.81: average alpha across the 16 years of available data). Also, the eigenvalues of the “second-order” computation, seem to not suggest an effective lower dimensionality construct, as almost all the eigenvalues are higher than one. Finally, based on the results of the above factor analysis criterion, that of 5 factors for the six variables available, the loadings of each of the six variables for the 5 different factors suggests a clustering that leads to the grouping of “Education” (0.80), with the categories of “Infrastructure” (0.78), and that of “Social Conditions” (0.60), and with the remaining variables being assigned 1:1 to the second, third, and fourth factor {i.e., “Labor” (0.71), “Environment” (0.69), and Health (0,79)}. The above loadings apply for the first year of the available data, but this pattern does not hold in a consistent way for the following years. Based on that variation, an equal weights procedure was chosen for the “second-order” step.\n",
    "\n",
    "It should be noted that the “first-order” analysis (i.e., the aggregation by averaging over the variables across the categories) was is effective because all the measures used were relative to the population of the countries, and thus, comparable in level.\n",
    "\n",
    "To depict the answer to the posed research question as clearly as possible, data regarding the GDP per capita were imported. Then, out of the 50 countries, 16 of them were chosen as examples, ranging from low to high income. The final graphs, presenting the trend of the Wellbeing and Development Indicator in juxtaposition to the trend of the GDP per capita for each of the 16 countries, suggest on average that in countries considered as high income, the state of social cohesion and the adequacy of services decreases or remains constant, while in the medium or low-income countries it increases alongside the monetary measure of GDP per capita. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "2e95c3f2ef005a63968a0ade420f5af7",
     "grade": true,
     "grade_id": "cell-d84f19967b2baa1d",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "When it comes to the main assumptions that need to govern the data, while using the factor analysis statistical method and the statistical measure of Cronbach’s Alpha, are, for the former, that the correlations among the variables are not too high, that there are no outliers (due to its inability as a method to produce robust results), that there is a uniformity of the variables scale and their scale type need not be anything else other than interval or ratio, that there are linear correlations (an assumption that is too hard to be accepted as true for large groups of variables), and last but not least,  that there is no multicollinearity. Then, when it comes to the latter, there needs to be a unidimensionality for the variables used to construct a compounded index, in the absence of which a great underestimation of the reliability of the index could take place. Also, this underestimation applies in the presence (or more accurately absence) of a high enough number of items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing, Filtering and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3dd4c02198f7b81f943268cb3d0a12b2",
     "grade": true,
     "grade_id": "cell-a2231ee7e6d4e686",
     "locked": false,
     "points": 7,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nassos_R\\\\Desktop\\\\Applied Economic Analysis 1\\\\Final Assignment\\\\Variables\\\\'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-537fbaa14668>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_parent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m#Iterating through the folders of the given path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mfolder_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcsv_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_parent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m#Iterating through the .csv files contained in those folders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Nassos_R\\\\Desktop\\\\Applied Economic Analysis 1\\\\Final Assignment\\\\Variables\\\\'"
     ]
    }
   ],
   "source": [
    "# !pip install factor_analyzer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import statistics as st\n",
    "import itertools as it\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "\n",
    "\n",
    "\n",
    "path_to_parent= \"C:\\\\Users\\\\Nassos_R\\\\Desktop\\\\Applied Economic Analysis 1\\\\Final Assignment\\\\Variables\\\\\" # Local path\n",
    "paths=[]\n",
    "dfs=[]\n",
    "\n",
    "\n",
    "\n",
    "for folder in os.listdir(path_to_parent):   #Iterating through the folders of the given path\n",
    "    folder_name=folder\n",
    "    for csv_file in os.listdir(os.path.join(path_to_parent, folder)):  #Iterating through the .csv files contained in those folders\n",
    "        path=path_to_parent+folder_name+\"\\\\\"+csv_file    # Combining the parent path and the .csv file name\n",
    "        paths.append(path)   # Appending to a list the full path of every .csv file present in our working directory\n",
    "\n",
    "    \n",
    "\n",
    "for path in paths:\n",
    "    df=pd.read_csv(path)  # A dataframe is created for every .csv file of the directory...\n",
    "    dfs.append(df)   # ... and then it is \"saved\"/assigned to an aggregate list\n",
    "\n",
    "\n",
    "\n",
    "variables=[]\n",
    "for df in dfs:\n",
    "    variables.append(set(df['Variable']))  # For each dataframe, their \"Variable\" column is being assigned as a set to get rid of the duplicate values\n",
    "\n",
    "\n",
    "\n",
    "variables_list=[]\n",
    "for var_set in variables:\n",
    "    var_set=list(var_set)\n",
    "    variables_list.append(var_set) # Appending the sets as lists\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "total_var=[]\n",
    "for setv in variables:\n",
    " for i in setv:\n",
    "  total_var.append(i) # Final List of All the Variables Used in the Analysis\n",
    "\n",
    "\n",
    "\n",
    "num_set=[]\n",
    "for setv in variables:\n",
    "  num_set.append(len(setv))\n",
    "\n",
    "\n",
    "\n",
    "countries=[]\n",
    "for df in dfs:\n",
    "    countries.append(set(df['Country'])) # The same as above applies here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "list_countries=[]\n",
    "for setc in countries:\n",
    "    for i in setc:\n",
    "     list_countries.append(i) # And here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inter_list=[]\n",
    "inter=set.intersection(*map(set,countries))\n",
    "for i in inter:\n",
    " inter_list.append(i) # Creating a Final List of the Available/Intersecting Countries Across all the Datasets\n",
    "\n",
    "\n",
    "inter_list.sort()\n",
    "\n",
    "\n",
    "\n",
    "filtered_by_country=[]\n",
    "for df in dfs:\n",
    "    df=df[df['Country'].isin(inter_list)]\n",
    "    filtered_by_country.append(df) # The same Data Frames but filtered based on the intersecting countries list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "years=range(2000,2016,1)\n",
    "filtered=[]\n",
    "for df in filtered_by_country:\n",
    "    df=df[df['Year'].isin(years)]\n",
    "    filtered.append(df)  # Filtered for the period of interest\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "dfs_index =[]\n",
    "for df in filtered:\n",
    "    date_var=pd.to_datetime(df['Year'], format='%Y')\n",
    "    dfs_index.append(df.set_index(date_var)) # Assigning indexes based on the \"Year\" Column of the Data as Dates\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "iso_c=[]\n",
    "for countr in inter_list:\n",
    " for df in dfs_index:\n",
    "     df=df.loc[df['Country']==countr]\n",
    "     iso_c.append(df) # Isolating 33*50 Dataframes (Time Series) for each Variable (#33) and Country (#50)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "var_cat=np.array_split(iso_c,50) # Splitting those Dataframes to chunks of 50, where each one contains the dataframes all variables for one country\n",
    "\n",
    "\n",
    "\n",
    "var_dfs=[]\n",
    "for arr in var_cat:\n",
    " for df in arr:\n",
    "  df=df.resample('Y').ffill(limit=1).interpolate() # Filling in missing values\n",
    "  df['Country'].fillna(method='ffill',inplace=True) # Filling in missing string values occuring by the new lines created from the interpolation\n",
    "  df['Variable'].fillna(method='ffill',inplace=True) # Same\n",
    "  var_dfs.append(df) # New list of interpolated data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for df in var_dfs: \n",
    " for col in df.columns:  # Removing the columns that are not intersesting with all the datasets in order to create uniform tables across the variables\n",
    "  if col=='Country Code':\n",
    "   del df[col]\n",
    "  elif col=='Series Code':\n",
    "   del df[col] \n",
    "  elif col=='Time Code':\n",
    "   del df[col]\n",
    "  elif col=='Code':\n",
    "   del df[col]\n",
    "  elif col=='Year Code':\n",
    "   del df[col]\n",
    "  else:\n",
    "   pass\n",
    "\n",
    "\n",
    "\n",
    "merged=pd.concat(var_dfs) # Merging all the datapoints available, that is, 33 Variables x 50 Countries x 16 years, or 26400 rows of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3dd4c02198f7b81f943268cb3d0a12b2",
     "grade": true,
     "grade_id": "cell-a2231ee7e6d4e686",
     "locked": false,
     "points": 7,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "## Categorization of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_form=pd.DataFrame(index=inter_list,columns=total_var) # Creating an empty dataframe layout to fill it in with the respective data based on the variables list from the first section\n",
    "yearly_data=[]\n",
    "for i in range(1,17):  # Creating an empty panel based on the layout created above\n",
    "    yearly_data.append(init_form.copy()) # The copy() method is not positioned here by chance. If we were to not include it, every value assigned in the next steps would be broadcasted to the same coordinates of the other tables\n",
    "\n",
    "\n",
    "\n",
    "for index, row in merged.iterrows(): # Based on the uniform and aggregate table, we assign the values relating to the given Year, Country, and Variable, for all 16 years\n",
    "  if row['Year']==2000:\n",
    "    yearly_data[0].loc[yearly_data[0].index==row['Country'],row['Variable']]=row['Value']\n",
    "  elif row['Year']==2001:  \n",
    "    yearly_data[1].loc[yearly_data[1].index==row['Country'],row['Variable']]=row['Value']\n",
    "  elif row['Year']==2002:  \n",
    "    yearly_data[2].loc[yearly_data[2].index==row['Country'],row['Variable']]=row['Value']\n",
    "  elif row['Year']==2003:  \n",
    "    yearly_data[3].loc[yearly_data[3].index==row['Country'],row['Variable']]=row['Value']\n",
    "  elif row['Year']==2004:  \n",
    "    yearly_data[4].loc[yearly_data[4].index==row['Country'],row['Variable']]=row['Value'] \n",
    "  elif row['Year']==2005:  \n",
    "    yearly_data[5].loc[yearly_data[5].index==row['Country'],row['Variable']]=row['Value']\n",
    "  elif row['Year']==2006:  \n",
    "    yearly_data[6].loc[yearly_data[6].index==row['Country'],row['Variable']]=row['Value']\n",
    "  elif row['Year']==2007:  \n",
    "    yearly_data[7].loc[yearly_data[7].index==row['Country'],row['Variable']]=row['Value']\n",
    "  elif row['Year']==2008:  \n",
    "    yearly_data[8].loc[yearly_data[8].index==row['Country'],row['Variable']]=row['Value']\n",
    "  elif row['Year']==2009:  \n",
    "    yearly_data[9].loc[yearly_data[9].index==row['Country'],row['Variable']]=row['Value']\n",
    "  elif row['Year']==2010:  \n",
    "    yearly_data[10].loc[yearly_data[10].index==row['Country'],row['Variable']]=row['Value']\n",
    "  elif row['Year']==2011:  \n",
    "    yearly_data[11].loc[yearly_data[11].index==row['Country'],row['Variable']]=row['Value']\n",
    "  elif row['Year']==2012:  \n",
    "    yearly_data[12].loc[yearly_data[12].index==row['Country'],row['Variable']]=row['Value']\n",
    "  elif row['Year']==2013:  \n",
    "    yearly_data[13].loc[yearly_data[13].index==row['Country'],row['Variable']]=row['Value']\n",
    "  elif row['Year']==2014:  \n",
    "    yearly_data[14].loc[yearly_data[14].index==row['Country'],row['Variable']]=row['Value']\n",
    "  else:\n",
    "    yearly_data[15].loc[yearly_data[15].index==row['Country'],row['Variable']]=row['Value']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-25944cd91a77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msorted_dfs_yv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmerged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Variable'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mergesort'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnew_arr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted_dfs_yv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m528\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minit_arr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m528\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_arr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_arr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'merged' is not defined"
     ]
    }
   ],
   "source": [
    "sorted_dfs_yv=merged.sort_index().sort_values('Variable', kind='mergesort')\n",
    "new_arr=np.array_split(sorted_dfs_yv,528)\n",
    "init_arr=np.ones([528,50]) \n",
    "for arr,x in it.product(new_arr,range(0,len(new_arr))):\n",
    "    for pos,y in it.product(arr,range(0,len(arr))):\n",
    "        new_p=new_arr[x]['Value'][y]\n",
    "        init_arr[x][y]=new_p # Creating 528 arrays for 16 x 33 variables containing the yearly values of a certain variable for each year --> Cross-sectional analysis\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "mean_table={'Year':[],'Variable':[],'Mean':[]}\n",
    "\n",
    "\n",
    "for arr in new_arr:\n",
    "    mean_table['Mean'].append(arr['Value'].mean())\n",
    "    mean_table['Variable'].append(arr['Variable'][1])\n",
    "    mean_table['Year'].append(arr['Year'][1])\n",
    "\n",
    "\n",
    "mean_df=pd.DataFrame(mean_table) # Mean per year across Countries and Variables\n",
    "\n",
    "\n",
    "\n",
    "var_table={'Year':[],'Variable':[],'Variance':[]}\n",
    "\n",
    "\n",
    "\n",
    "for arr in new_arr:\n",
    "    var_table['Variance'].append(arr['Value'].var())\n",
    "    var_table['Variable'].append(arr['Variable'][1])\n",
    "    var_table['Year'].append(arr['Year'][1])\n",
    "\n",
    "\n",
    "\n",
    "var_df=pd.DataFrame(var_table) # Same for Variance\n",
    "\n",
    "\n",
    "\n",
    "sd_table={'Year':[],'Variable':[],'Standard Deviation':[]}\n",
    "\n",
    "\n",
    "\n",
    "for arr in new_arr:\n",
    "    sd_table['Standard Deviation'].append(arr['Value'].std())\n",
    "    sd_table['Variable'].append(arr['Variable'][1])\n",
    "    sd_table['Year'].append(arr['Year'][1])\n",
    "\n",
    "\n",
    "\n",
    "sd_df=pd.DataFrame(sd_table) # Same for Standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cronbach's Coefficient Alpha and Reverse Scoring (Individual Indexes/\"First-order\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oposs=[False, False, False, True, True, False, True, True, False, True, False, True, True, True, False, True,\n",
    "True, False, False, False, True, True, True, False, True, True, False, False, False, False, False, False, True] # True values indicate the positions of variables that have an opposite direction in relation to all the other variables.\n",
    "\n",
    "\n",
    "totvar_arr=np.array(total_var) \n",
    "\n",
    "oposs_arr=np.array(oposs)\n",
    "\n",
    "rev_arr=totvar_arr[oposs_arr] # Filtered variables containing only those of the opposite direction using mask method\n",
    "\n",
    "\n",
    "\n",
    "yearly_oposs=yearly_data.copy() # Using copy() for he same reason that was mentioned earlier\n",
    "\n",
    "\n",
    "for df in yearly_oposs:\n",
    " for colum in rev_arr: # For the columns/variables that have the opposite direction qualitatively...\n",
    "  max_var=df[colum]  # ... we save their maximum value...\n",
    "  for index, row in df.iterrows():\n",
    "     row[colum]=max(max_var)-row[colum] # ... and then, we deduct from the given maximum value, the value already present in the given cell\n",
    "\n",
    "\n",
    "\n",
    "cat_yearly=[]\n",
    "for i in range(0,len(yearly_data)):\n",
    "    edu=yearly_oposs[i].iloc[:,0:3]\n",
    "    envi=yearly_oposs[i].iloc[:,3:6]\n",
    "    health=yearly_oposs[i].iloc[:,6:17]\n",
    "    inf=yearly_oposs[i].iloc[:,17:20]\n",
    "    lab=yearly_oposs[i].iloc[:,20:23]\n",
    "    social=yearly_oposs[i].iloc[:,23:33]\n",
    "    wellb_indi=[edu,envi,health,inf,lab,social]\n",
    "    cat_yearly.append(wellb_indi) # Creating a list of lists of dataframes based on the manually created categories\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "correl=[]\n",
    "N=[]\n",
    "for year in cat_yearly:\n",
    "  for indi in year:\n",
    "    correl.append(indi.astype(float).corr()) # Yearly correlation for the variables of each category \n",
    "    N.append(indi.shape[1]) \n",
    "\n",
    "\n",
    "correl=np.array_split(correl,16)\n",
    "N=np.array_split(N,16)\n",
    "\n",
    "\n",
    "alpha_ind=[]\n",
    "for x in range(0,len(cat_yearly)): # Based on the panel data length\n",
    " for k in range(0,len(cat_yearly[x])): # Based on the categories assigned\n",
    "   sum_arr = np.array([]) # Used to sum the correlation for each combination of variables in each category\n",
    "   for i, col in enumerate(correl[x][k].columns): # 16 years, 6 categories per year\n",
    "        interm = correl[x][k][col][i+1:].values # for each row the correlation that we need to append and aggregate into the final table in orded to get the mean for the given year and category is always the index of the row +1. For example, for variable 5 positioned in index 4 the correlation with variable 6 would be positioned horizontally on column var5_index+1\n",
    "        sum_arr = np.append(interm, sum_arr)        \n",
    "   mean_corr = np.mean(sum_arr) # Mean correlation of the given category of a given year\n",
    "   c_alpha = (N[x][k]*mean_corr)/(1+(N[x][k]-1)*mean_corr) # Alpha Coefficient Function\n",
    "   alpha_ind.append(c_alpha) \n",
    "\n",
    "\n",
    "alpha_ind=np.array_split(alpha_ind,16)\n",
    "\n",
    "mean_alpha=[]\n",
    "for i in range(0,6):\n",
    "  mean=(alpha_ind[0][i]+alpha_ind[1][i]+alpha_ind[2][i]+alpha_ind[3][i]+alpha_ind[4][i]+alpha_ind[5][i]+alpha_ind[6][i]+alpha_ind[7][i]+alpha_ind[8][i]+alpha_ind[9][i]+alpha_ind[10][i]+alpha_ind[11][i]+alpha_ind[12][i]+alpha_ind[13][i]+alpha_ind[14][i]+alpha_ind[15][i])/16\n",
    "  mean_alpha.append(mean)  # Average alpha coefficient across 16 years for each category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor Analysis (Individual Indexes/\"First-order\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eigenvalues(df,x): # Function that returns the eigenvalues for a given number of factors\n",
    "    fa=FactorAnalyzer(x, rotation=None)\n",
    "    fa.fit(df)\n",
    "    eigen, val=fa.get_eigenvalues()\n",
    "    return(val)\n",
    "\n",
    "\n",
    "\n",
    "eigenval=[]\n",
    "for year in cat_yearly: # By year\n",
    " for indi in year: # By category\n",
    "    eigenval.append(eigenvalues(indi,32))\n",
    "\n",
    "\n",
    "eigen_split=np.array_split(np.array(eigenval),16) # Eigenvalues for each category across 16 years\n",
    "    \n",
    "\n",
    "loadings=[]\n",
    "for year in cat_yearly:\n",
    " for indi in year:\n",
    "     fa=FactorAnalyzer(3, rotation='varimax') # Loadings for 3 factors\n",
    "     fa.fit(indi)\n",
    "     loadings.append(fa.loadings_) \n",
    "    \n",
    "    \n",
    "    \n",
    "load_split=np.array_split(np.array(loadings),16) # Loadings for each category across 16 years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"First-order\" Compound Indexes (Equal Weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_index=pd.DataFrame(index=cat_yearly[0][0].index,columns=['Education','Environment','Health','Infrastructure','Labor','Social Conditions']) # New empty layout based on the major categories\n",
    "\n",
    "\n",
    "\n",
    "yearly_oposs_final=[]\n",
    "for i in range(0,16):\n",
    "     yearly_oposs_final.append(single_index.copy()) # Distributing the empty layout across 16 years\n",
    "\n",
    "\n",
    "for x in range(0,16): # By year\n",
    " for y in range(0,6): # By category\n",
    "  for index, row in cat_yearly[x][y].iterrows(): # For each row of a given year and category we assign the mean value to the singular positions of the empty layout of the major categories\n",
    "          if y==0: # Conditional based on the category that we are in\n",
    "              mean=st.mean(row) # The mean values of a given row, in so, equal weights are automatically being assigned\n",
    "              yearly_oposs_final[x].loc[yearly_oposs_final[x].index==row.name,'Education']=mean\n",
    "          elif y==1:\n",
    "              mean=st.mean(row)\n",
    "              yearly_oposs_final[x].loc[yearly_oposs_final[x].index==row.name,'Environment']=mean\n",
    "          elif y==2:\n",
    "              mean=st.mean(row)\n",
    "              yearly_oposs_final[x].loc[yearly_oposs_final[x].index==row.name,'Health']=mean\n",
    "          elif y==3:\n",
    "              mean=st.mean(row)\n",
    "              yearly_oposs_final[x].loc[yearly_oposs_final[x].index==row.name,'Infrastructure']=mean\n",
    "          elif y==4:\n",
    "              mean=st.mean(row)\n",
    "              yearly_oposs_final[x].loc[yearly_oposs_final[x].index==row.name,'Labor']=mean\n",
    "          else:\n",
    "              mean=st.mean(row)\n",
    "              yearly_oposs_final[x].loc[yearly_oposs_final[x].index==row.name,'Social Conditions']=mean\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Second-order\" Conbrach's Coefficient Alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correl_oposs_final=[]\n",
    "N=[]\n",
    "for indi in yearly_oposs_final:\n",
    "    correl_oposs_final.append(indi.astype(float).corr())\n",
    "    N.append(indi.shape[1])\n",
    "\n",
    "\n",
    "\n",
    "alpha_ind_final=[]\n",
    "for k in range(0,len(yearly_oposs_final)):\n",
    " sum_arr = np.array([])\n",
    " for i, col in enumerate(correl_oposs_final[k].columns):\n",
    "        interm = correl_oposs_final[k][col][i+1:].values\n",
    "        sum_arr = np.append(interm, sum_arr)        \n",
    " mean_corr = np.mean(sum_arr)\n",
    " c_alpha = (N[k]*mean_corr)/(1+(N[k]-1)*mean_corr)\n",
    " alpha_ind_final.append(c_alpha)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mean_alpha_final=(alpha_ind_final[0]+alpha_ind_final[1]+alpha_ind_final[2]+alpha_ind_final[3]+alpha_ind_final[4]+alpha_ind_final[5]+alpha_ind_final[6]+alpha_ind_final[7]+alpha_ind_final[8]+alpha_ind_final[9]+alpha_ind_final[10]+alpha_ind_final[11]+alpha_ind_final[12]+alpha_ind_final[13]+alpha_ind_final[14]+alpha_ind_final[15])/16\n",
    "print(mean_alpha_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Second-order\" Factor Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenval_final=[]\n",
    "for year in yearly_oposs_final:\n",
    "    eigenval_final.append(eigenvalues(year,20))\n",
    "\n",
    "\n",
    "eigen_split_final=np.array_split(np.array(eigenval_final),16)\n",
    "    \n",
    "\n",
    "loadings_final=[]\n",
    "\n",
    "for year in yearly_oposs_final:\n",
    "     fa=FactorAnalyzer(5, rotation='varimax')\n",
    "     fa.fit(year)\n",
    "     loadings_final.append(fa.loadings_)\n",
    "\n",
    "\n",
    "    \n",
    "load_split_final=np.array_split(np.array(loadings_final),16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second-order Index: Wellbeing and Development Indicator (Equal Weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cat_yearly' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-fbed6261f13e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwellb_table\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcat_yearly\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'2000'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2001'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2002'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2003'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2004'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2005'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2006'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2007'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2008'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2009'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2010'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2011'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2012'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2013'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2014'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2015'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Final empty layout for the 16 years and 50 countries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# By year\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cat_yearly' is not defined"
     ]
    }
   ],
   "source": [
    "wellb_table=pd.DataFrame(index=cat_yearly[0][0].index,columns=['2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015']) # Final empty layout for the 16 years and 50 countries\n",
    "\n",
    "\n",
    "\n",
    "for x in range(0,16):  # By year\n",
    "  for index, row in yearly_oposs_final[x].iterrows(): \n",
    "          if x==0:\n",
    "              mean=st.mean(row) # Equal weights across categories\n",
    "              wellb_table.loc[wellb_table.index==row.name,'2000']=mean\n",
    "          elif x==1:\n",
    "              mean=st.mean(row)\n",
    "              wellb_table.loc[wellb_table.index==row.name,'2001']=mean\n",
    "          elif x==2:\n",
    "              mean=st.mean(row)\n",
    "              wellb_table.loc[wellb_table.index==row.name,'2002']=mean\n",
    "          elif x==3:\n",
    "              mean=st.mean(row)\n",
    "              wellb_table.loc[wellb_table.index==row.name,'2003']=mean\n",
    "          elif x==4:\n",
    "              mean=st.mean(row)\n",
    "              wellb_table.loc[wellb_table.index==row.name,'2004']=mean\n",
    "          elif x==5:\n",
    "              mean=st.mean(row)\n",
    "              wellb_table.loc[wellb_table.index==row.name,'2005']=mean\n",
    "          elif x==6:\n",
    "              mean=st.mean(row)\n",
    "              wellb_table.loc[wellb_table.index==row.name,'2006']=mean\n",
    "          elif x==7:\n",
    "              mean=st.mean(row)\n",
    "              wellb_table.loc[wellb_table.index==row.name,'2007']=mean\n",
    "          elif x==8:\n",
    "              mean=st.mean(row)\n",
    "              wellb_table.loc[wellb_table.index==row.name,'2008']=mean\n",
    "          elif x==9:\n",
    "              mean=st.mean(row)\n",
    "              wellb_table.loc[wellb_table.index==row.name,'2009']=mean\n",
    "          elif x==10:\n",
    "              mean=st.mean(row)\n",
    "              wellb_table.loc[wellb_table.index==row.name,'2010']=mean\n",
    "          elif x==11:\n",
    "              mean=st.mean(row)\n",
    "              wellb_table.loc[wellb_table.index==row.name,'2011']=mean\n",
    "          elif x==12:\n",
    "              mean=st.mean(row)\n",
    "              wellb_table.loc[wellb_table.index==row.name,'2012']=mean\n",
    "          elif x==13:\n",
    "              mean=st.mean(row)\n",
    "              wellb_table.loc[wellb_table.index==row.name,'2013']=mean\n",
    "          elif x==14:\n",
    "              mean=st.mean(row)\n",
    "              wellb_table.loc[wellb_table.index==row.name,'2014']=mean              \n",
    "          else:\n",
    "              mean=st.mean(row)\n",
    "              wellb_table.loc[wellb_table.index==row.name,'2015']=mean\n",
    "            \n",
    "\n",
    "            \n",
    "wellb_table=wellb_table.T  # Transposing final table --> Years as rows\n",
    "            \n",
    "gdp=pd.read_csv(\"C:\\\\Users\\\\Nassos_R\\\\Desktop\\\\Applied Economic Analysis 1\\\\Final Assignment\\\\GDPpercapitaPPP.csv\")         \n",
    "            \n",
    "gdp_df=pd.DataFrame(index=range(2000,2016))\n",
    "\n",
    "for country in inter_list:\n",
    "    gdp_df[country]=None # Empty layout for GDP data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for index, row in gdp.iterrows():\n",
    "          if row.Year==2000:\n",
    "              gdp_df.loc[gdp_df.index==2000,row['Country']]=row.Value # Assigning GDP values for the given year and country in the respective coordinated of the empty layout\n",
    "          elif row.Year==2001:\n",
    "              gdp_df.loc[gdp_df.index==2001,row['Country']]=row.Value\n",
    "          elif row.Year==2002:\n",
    "              gdp_df.loc[gdp_df.index==2002,row['Country']]=row.Value\n",
    "          elif row.Year==2003:\n",
    "              gdp_df.loc[gdp_df.index==2003,row['Country']]=row.Value\n",
    "          elif row.Year==2004:\n",
    "              gdp_df.loc[gdp_df.index==2004,row['Country']]=row.Value\n",
    "          elif row.Year==2005:\n",
    "              gdp_df.loc[gdp_df.index==2005,row['Country']]=row.Value\n",
    "          elif row.Year==2006:\n",
    "              gdp_df.loc[gdp_df.index==2006,row['Country']]=row.Value\n",
    "          elif row.Year==2007:\n",
    "              gdp_df.loc[gdp_df.index==2007,row['Country']]=row.Value\n",
    "          elif row.Year==2008:\n",
    "              gdp_df.loc[gdp_df.index==2008,row['Country']]=row.Value\n",
    "          elif row.Year==2009:\n",
    "              gdp_df.loc[gdp_df.index==2009,row['Country']]=row.Value\n",
    "          elif row.Year==2010:\n",
    "              gdp_df.loc[gdp_df.index==2010,row['Country']]=row.Value\n",
    "          elif row.Year==2011:\n",
    "              gdp_df.loc[gdp_df.index==2011,row['Country']]=row.Value\n",
    "          elif row.Year==2012:\n",
    "              gdp_df.loc[gdp_df.index==2012,row['Country']]=row.Value\n",
    "          elif row.Year==2013:\n",
    "              gdp_df.loc[gdp_df.index==2013,row['Country']]=row.Value\n",
    "          elif row.Year==2014:\n",
    "              gdp_df.loc[gdp_df.index==2014,row['Country']]=row.Value             \n",
    "          else:\n",
    "              gdp_df.loc[gdp_df.index==2015,row['Country']]=row.Value\n",
    "\n",
    "\n",
    "                \n",
    "            \n",
    "wellb_table = wellb_table.set_index(pd.to_datetime(wellb_table.index))       \n",
    "gdp_df = gdp_df.set_index(wellb_table.index)         \n",
    "    \n",
    "    \n",
    "    \n",
    "fig, axs = plt.subplots(4, 4) # Panel of time series graphs of 16 selected countries --> Contrasting GDP to Wellbeing Index trend\n",
    "fig.tight_layout()\n",
    "\n",
    "\n",
    "plt.subplot(441)\n",
    "plt.xlabel('Argentina')         \n",
    "gdp_df.Argentina.plot(grid=True, label=\"GDP\", legend=True)\n",
    "wellb_table.Argentina.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(442)\n",
    "plt.xlabel('Brazil')         \n",
    "gdp_df.Brazil.plot(grid=True, label=\"GDP\", legend=True)\n",
    "wellb_table.Brazil.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(443)\n",
    "plt.xlabel('Chile')         \n",
    "gdp_df.Chile.plot(grid=True, label=\"GDP\", legend=True)\n",
    "wellb_table.Chile.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(444)\n",
    "plt.xlabel('France')         \n",
    "gdp_df.France.plot(grid=True, label=\"GDP\", legend=True)\n",
    "wellb_table.France.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(445)\n",
    "plt.xlabel('Ireland')         \n",
    "gdp_df.Ireland.plot(grid=True, label=\"GDP\", legend=True)\n",
    "wellb_table.Ireland.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(446)\n",
    "plt.xlabel('Italy')         \n",
    "gdp_df.Italy.plot(grid=True, label=\"GDP\", legend=True)\n",
    "wellb_table.Italy.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(447)\n",
    "plt.xlabel('Latvia')         \n",
    "gdp_df.Latvia.plot(grid=True, label=\"GDP\", legend=True)\n",
    "wellb_table.Latvia.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(448)\n",
    "plt.xlabel('Lithuania')         \n",
    "gdp_df.Lithuania.plot(grid=True, label=\"GDP\", legend=True)\n",
    "wellb_table.Lithuania.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(4,4,9)\n",
    "plt.xlabel('Malaysia')         \n",
    "gdp_df.Malaysia.plot(grid=True, label=\"GDP\", legend=True)\n",
    "wellb_table.Malaysia.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(4,4,10)\n",
    "plt.xlabel('Netherlands')         \n",
    "gdp_df.Netherlands.plot(grid=True, label=\"GDP\", legend=True)\n",
    "wellb_table.Netherlands.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(4,4,11)\n",
    "plt.xlabel('Pakistan')         \n",
    "gdp_df.Pakistan.plot(grid=True, label=\"GDP\", legend=True)\n",
    "wellb_table.Pakistan.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(4,4,12)\n",
    "plt.xlabel('Peru')         \n",
    "gdp_df.Peru.plot(grid=True, label=\"GDP\", legend=True)\n",
    "wellb_table.Peru.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(4,4,13)\n",
    "plt.xlabel('Spain')         \n",
    "gdp_df.Spain.plot(grid=True, label=\"GDP\", legend=True)\n",
    "wellb_table.Spain.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(4,4,14)\n",
    "plt.xlabel('Switzerland')         \n",
    "gdp_df.Switzerland.plot(grid=True, label=\"GDP\", legend=True)\n",
    "wellb_table.Switzerland.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(4,4,15)\n",
    "plt.xlabel('Thailand')         \n",
    "gdp_df.Thailand.plot(grid=True, label=\"GDP\", legend=True)\n",
    "wellb_table.Thailand.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.subplot(4,4,16)\n",
    "plt.xlabel('United States')         \n",
    "gdp_df['United States'].plot(grid=True, label=\"GDP\", legend=True)\n",
    "wellb_table['United States'].plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitivity analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test for the sensitivity of the final results, each of the first-order indexes was removed incrementally, to construct six new tables containing the respective values of the second-order index, or, in other words, the Wellbeing and Development Index. The sensitivity test suggests that the category of most importance is the Labor Conditions, while for all the remaining categories, the trend remains almost unchanged. In the following paragraph of Discussion and Conclusions, it is suggested that several crucial reconfigurations should take place if the present analysis was to be revisited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "checksum": "3dd4c02198f7b81f943268cb3d0a12b2",
     "grade": true,
     "grade_id": "cell-a2231ee7e6d4e686",
     "locked": false,
     "points": 7,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "yearly_noedu=[] # Creating new final tables where one out of the six main categories is being removed incrementally\n",
    "yearly_noenvi=[]\n",
    "yearly_nohealth=[]\n",
    "yearly_noinf=[]\n",
    "yearly_nolab=[]\n",
    "yearly_nosoc=[]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "copy_final1=yearly_oposs_final[0].copy(deep=True) # Copy() method for the exact same reason\n",
    "copy_final2=yearly_oposs_final[1].copy(deep=True)\n",
    "copy_final3=yearly_oposs_final[2].copy(deep=True)\n",
    "copy_final4=yearly_oposs_final[3].copy(deep=True)\n",
    "copy_final5=yearly_oposs_final[4].copy(deep=True)\n",
    "copy_final6=yearly_oposs_final[5].copy(deep=True)\n",
    "copy_final7=yearly_oposs_final[6].copy(deep=True)\n",
    "copy_final8=yearly_oposs_final[7].copy(deep=True)\n",
    "copy_final9=yearly_oposs_final[8].copy(deep=True)\n",
    "copy_final10=yearly_oposs_final[9].copy(deep=True)\n",
    "copy_final11=yearly_oposs_final[10].copy(deep=True)\n",
    "copy_final12=yearly_oposs_final[11].copy(deep=True)\n",
    "copy_final13=yearly_oposs_final[12].copy(deep=True)\n",
    "copy_final14=yearly_oposs_final[13].copy(deep=True)\n",
    "copy_final15=yearly_oposs_final[14].copy(deep=True)\n",
    "copy_final16=yearly_oposs_final[15].copy(deep=True)\n",
    "\n",
    "\n",
    "copy_agg=[copy_final1,copy_final2,copy_final3,copy_final4,copy_final5,copy_final6,copy_final7,copy_final8,copy_final9,copy_final10,copy_final11,copy_final12,\n",
    "          copy_final13,copy_final14,copy_final15,copy_final16]\n",
    "\n",
    "\n",
    "copy_final=copy_agg.copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for df in copy_final:\n",
    "    del(df['Education'])\n",
    "    yearly_noedu.append(df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "copy_final1=yearly_oposs_final[0].copy(deep=True)\n",
    "copy_final2=yearly_oposs_final[1].copy(deep=True)\n",
    "copy_final3=yearly_oposs_final[2].copy(deep=True)\n",
    "copy_final4=yearly_oposs_final[3].copy(deep=True)\n",
    "copy_final5=yearly_oposs_final[4].copy(deep=True)\n",
    "copy_final6=yearly_oposs_final[5].copy(deep=True)\n",
    "copy_final7=yearly_oposs_final[6].copy(deep=True)\n",
    "copy_final8=yearly_oposs_final[7].copy(deep=True)\n",
    "copy_final9=yearly_oposs_final[8].copy(deep=True)\n",
    "copy_final10=yearly_oposs_final[9].copy(deep=True)\n",
    "copy_final11=yearly_oposs_final[10].copy(deep=True)\n",
    "copy_final12=yearly_oposs_final[11].copy(deep=True)\n",
    "copy_final13=yearly_oposs_final[12].copy(deep=True)\n",
    "copy_final14=yearly_oposs_final[13].copy(deep=True)\n",
    "copy_final15=yearly_oposs_final[14].copy(deep=True)\n",
    "copy_final16=yearly_oposs_final[15].copy(deep=True)\n",
    "\n",
    "\n",
    "copy_agg=[copy_final1,copy_final2,copy_final3,copy_final4,copy_final5,copy_final6,copy_final7,copy_final8,copy_final9,copy_final10,copy_final11,copy_final12,\n",
    "          copy_final13,copy_final14,copy_final15,copy_final16]\n",
    "\n",
    "\n",
    "copy_final=copy_agg.copy()\n",
    "\n",
    "\n",
    "for df in copy_final:\n",
    "    del(df['Environment'])\n",
    "    yearly_noenvi.append(df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "copy_final1=yearly_oposs_final[0].copy(deep=True)\n",
    "copy_final2=yearly_oposs_final[1].copy(deep=True)\n",
    "copy_final3=yearly_oposs_final[2].copy(deep=True)\n",
    "copy_final4=yearly_oposs_final[3].copy(deep=True)\n",
    "copy_final5=yearly_oposs_final[4].copy(deep=True)\n",
    "copy_final6=yearly_oposs_final[5].copy(deep=True)\n",
    "copy_final7=yearly_oposs_final[6].copy(deep=True)\n",
    "copy_final8=yearly_oposs_final[7].copy(deep=True)\n",
    "copy_final9=yearly_oposs_final[8].copy(deep=True)\n",
    "copy_final10=yearly_oposs_final[9].copy(deep=True)\n",
    "copy_final11=yearly_oposs_final[10].copy(deep=True)\n",
    "copy_final12=yearly_oposs_final[11].copy(deep=True)\n",
    "copy_final13=yearly_oposs_final[12].copy(deep=True)\n",
    "copy_final14=yearly_oposs_final[13].copy(deep=True)\n",
    "copy_final15=yearly_oposs_final[14].copy(deep=True)\n",
    "copy_final16=yearly_oposs_final[15].copy(deep=True)\n",
    "\n",
    "\n",
    "copy_agg=[copy_final1,copy_final2,copy_final3,copy_final4,copy_final5,copy_final6,copy_final7,copy_final8,copy_final9,copy_final10,copy_final11,copy_final12,\n",
    "          copy_final13,copy_final14,copy_final15,copy_final16]\n",
    "\n",
    "\n",
    "copy_final=copy_agg.copy()\n",
    "\n",
    "for df in copy_final:\n",
    "    del(df['Health'])\n",
    "    yearly_nohealth.append(df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "copy_final1=yearly_oposs_final[0].copy(deep=True)\n",
    "copy_final2=yearly_oposs_final[1].copy(deep=True)\n",
    "copy_final3=yearly_oposs_final[2].copy(deep=True)\n",
    "copy_final4=yearly_oposs_final[3].copy(deep=True)\n",
    "copy_final5=yearly_oposs_final[4].copy(deep=True)\n",
    "copy_final6=yearly_oposs_final[5].copy(deep=True)\n",
    "copy_final7=yearly_oposs_final[6].copy(deep=True)\n",
    "copy_final8=yearly_oposs_final[7].copy(deep=True)\n",
    "copy_final9=yearly_oposs_final[8].copy(deep=True)\n",
    "copy_final10=yearly_oposs_final[9].copy(deep=True)\n",
    "copy_final11=yearly_oposs_final[10].copy(deep=True)\n",
    "copy_final12=yearly_oposs_final[11].copy(deep=True)\n",
    "copy_final13=yearly_oposs_final[12].copy(deep=True)\n",
    "copy_final14=yearly_oposs_final[13].copy(deep=True)\n",
    "copy_final15=yearly_oposs_final[14].copy(deep=True)\n",
    "copy_final16=yearly_oposs_final[15].copy(deep=True)\n",
    "\n",
    "\n",
    "copy_agg=[copy_final1,copy_final2,copy_final3,copy_final4,copy_final5,copy_final6,copy_final7,copy_final8,copy_final9,copy_final10,copy_final11,copy_final12,\n",
    "          copy_final13,copy_final14,copy_final15,copy_final16]\n",
    "\n",
    "\n",
    "copy_final=copy_agg.copy()\n",
    "\n",
    "\n",
    "\n",
    "for df in copy_final:\n",
    "    del(df['Infrastructure'])\n",
    "    yearly_noinf.append(df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "copy_final1=yearly_oposs_final[0].copy(deep=True)\n",
    "copy_final2=yearly_oposs_final[1].copy(deep=True)\n",
    "copy_final3=yearly_oposs_final[2].copy(deep=True)\n",
    "copy_final4=yearly_oposs_final[3].copy(deep=True)\n",
    "copy_final5=yearly_oposs_final[4].copy(deep=True)\n",
    "copy_final6=yearly_oposs_final[5].copy(deep=True)\n",
    "copy_final7=yearly_oposs_final[6].copy(deep=True)\n",
    "copy_final8=yearly_oposs_final[7].copy(deep=True)\n",
    "copy_final9=yearly_oposs_final[8].copy(deep=True)\n",
    "copy_final10=yearly_oposs_final[9].copy(deep=True)\n",
    "copy_final11=yearly_oposs_final[10].copy(deep=True)\n",
    "copy_final12=yearly_oposs_final[11].copy(deep=True)\n",
    "copy_final13=yearly_oposs_final[12].copy(deep=True)\n",
    "copy_final14=yearly_oposs_final[13].copy(deep=True)\n",
    "copy_final15=yearly_oposs_final[14].copy(deep=True)\n",
    "copy_final16=yearly_oposs_final[15].copy(deep=True)\n",
    "\n",
    "\n",
    "copy_agg=[copy_final1,copy_final2,copy_final3,copy_final4,copy_final5,copy_final6,copy_final7,copy_final8,copy_final9,copy_final10,copy_final11,copy_final12,\n",
    "          copy_final13,copy_final14,copy_final15,copy_final16]\n",
    "\n",
    "\n",
    "copy_final=copy_agg.copy()\n",
    "\n",
    "for df in copy_final:\n",
    "    del(df['Labor'])\n",
    "    yearly_nolab.append(df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "copy_final1=yearly_oposs_final[0].copy(deep=True)\n",
    "copy_final2=yearly_oposs_final[1].copy(deep=True)\n",
    "copy_final3=yearly_oposs_final[2].copy(deep=True)\n",
    "copy_final4=yearly_oposs_final[3].copy(deep=True)\n",
    "copy_final5=yearly_oposs_final[4].copy(deep=True)\n",
    "copy_final6=yearly_oposs_final[5].copy(deep=True)\n",
    "copy_final7=yearly_oposs_final[6].copy(deep=True)\n",
    "copy_final8=yearly_oposs_final[7].copy(deep=True)\n",
    "copy_final9=yearly_oposs_final[8].copy(deep=True)\n",
    "copy_final10=yearly_oposs_final[9].copy(deep=True)\n",
    "copy_final11=yearly_oposs_final[10].copy(deep=True)\n",
    "copy_final12=yearly_oposs_final[11].copy(deep=True)\n",
    "copy_final13=yearly_oposs_final[12].copy(deep=True)\n",
    "copy_final14=yearly_oposs_final[13].copy(deep=True)\n",
    "copy_final15=yearly_oposs_final[14].copy(deep=True)\n",
    "copy_final16=yearly_oposs_final[15].copy(deep=True)\n",
    "\n",
    "\n",
    "copy_agg=[copy_final1,copy_final2,copy_final3,copy_final4,copy_final5,copy_final6,copy_final7,copy_final8,copy_final9,copy_final10,copy_final11,copy_final12,\n",
    "          copy_final13,copy_final14,copy_final15,copy_final16]\n",
    "\n",
    "\n",
    "copy_final=copy_agg.copy()\n",
    "\n",
    "for df in copy_final:\n",
    "    del(df['Social Conditions'])\n",
    "    yearly_nosoc.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combination 1 (No Education)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noedu_table=pd.DataFrame(index=cat_yearly[0][0].index,columns=['2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015'])\n",
    "\n",
    "\n",
    "\n",
    "for x in range(0,16):   \n",
    "  for index, row in yearly_noedu[x].iterrows():\n",
    "          if x==0:\n",
    "              mean=st.mean(row)\n",
    "              noedu_table.loc[noedu_table.index==row.name,'2000']=mean\n",
    "          elif x==1:\n",
    "              mean=st.mean(row)\n",
    "              noedu_table.loc[noedu_table.index==row.name,'2001']=mean\n",
    "          elif x==2:\n",
    "              mean=st.mean(row)\n",
    "              noedu_table.loc[noedu_table.index==row.name,'2002']=mean\n",
    "          elif x==3:\n",
    "              mean=st.mean(row)\n",
    "              noedu_table.loc[noedu_table.index==row.name,'2003']=mean\n",
    "          elif x==4:\n",
    "              mean=st.mean(row)\n",
    "              noedu_table.loc[noedu_table.index==row.name,'2004']=mean\n",
    "          elif x==5:\n",
    "              mean=st.mean(row)\n",
    "              noedu_table.loc[noedu_table.index==row.name,'2005']=mean\n",
    "          elif x==6:\n",
    "              mean=st.mean(row)\n",
    "              noedu_table.loc[noedu_table.index==row.name,'2006']=mean\n",
    "          elif x==7:\n",
    "              mean=st.mean(row)\n",
    "              noedu_table.loc[noedu_table.index==row.name,'2007']=mean\n",
    "          elif x==8:\n",
    "              mean=st.mean(row)\n",
    "              noedu_table.loc[noedu_table.index==row.name,'2008']=mean\n",
    "          elif x==9:\n",
    "              mean=st.mean(row)\n",
    "              noedu_table.loc[noedu_table.index==row.name,'2009']=mean\n",
    "          elif x==10:\n",
    "              mean=st.mean(row)\n",
    "              noedu_table.loc[noedu_table.index==row.name,'2010']=mean\n",
    "          elif x==11:\n",
    "              mean=st.mean(row)\n",
    "              noedu_table.loc[noedu_table.index==row.name,'2011']=mean\n",
    "          elif x==12:\n",
    "              mean=st.mean(row)\n",
    "              noedu_table.loc[noedu_table.index==row.name,'2012']=mean\n",
    "          elif x==13:\n",
    "              mean=st.mean(row)\n",
    "              noedu_table.loc[noedu_table.index==row.name,'2013']=mean\n",
    "          elif x==14:\n",
    "              mean=st.mean(row)\n",
    "              noedu_table.loc[noedu_table.index==row.name,'2014']=mean              \n",
    "          else:\n",
    "              mean=st.mean(row)\n",
    "              noedu_table.loc[noedu_table.index==row.name,'2015']=mean\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "noedu_table=noedu_table.T           \n",
    "\n",
    "\n",
    "                \n",
    "            \n",
    "noedu_table = noedu_table.set_index(pd.to_datetime(noedu_table.index))            \n",
    "\n",
    "    \n",
    "    \n",
    "fig, axs = plt.subplots(4, 4)\n",
    "fig.tight_layout()\n",
    "\n",
    "\n",
    "plt.subplot(441)\n",
    "plt.xlabel('Argentina')         \n",
    "gdp_df.Argentina.plot(grid=True, label=\"GDP\", legend=True)\n",
    "noedu_table.Argentina.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(442)\n",
    "plt.xlabel('Brazil')         \n",
    "gdp_df.Brazil.plot(grid=True, label=\"GDP\", legend=True)\n",
    "noedu_table.Brazil.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(443)\n",
    "plt.xlabel('Chile')         \n",
    "gdp_df.Chile.plot(grid=True, label=\"GDP\", legend=True)\n",
    "noedu_table.Chile.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(444)\n",
    "plt.xlabel('France')         \n",
    "gdp_df.France.plot(grid=True, label=\"GDP\", legend=True)\n",
    "noedu_table.France.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(445)\n",
    "plt.xlabel('Ireland')         \n",
    "gdp_df.Ireland.plot(grid=True, label=\"GDP\", legend=True)\n",
    "noedu_table.Ireland.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(446)\n",
    "plt.xlabel('Italy')         \n",
    "gdp_df.Italy.plot(grid=True, label=\"GDP\", legend=True)\n",
    "noedu_table.Italy.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(447)\n",
    "plt.xlabel('Latvia')         \n",
    "gdp_df.Latvia.plot(grid=True, label=\"GDP\", legend=True)\n",
    "noedu_table.Latvia.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(448)\n",
    "plt.xlabel('Lithuania')         \n",
    "gdp_df.Lithuania.plot(grid=True, label=\"GDP\", legend=True)\n",
    "noedu_table.Lithuania.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(4,4,9)\n",
    "plt.xlabel('Malaysia')         \n",
    "gdp_df.Malaysia.plot(grid=True, label=\"GDP\", legend=True)\n",
    "noedu_table.Malaysia.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(4,4,10)\n",
    "plt.xlabel('Netherlands')         \n",
    "gdp_df.Netherlands.plot(grid=True, label=\"GDP\", legend=True)\n",
    "noedu_table.Netherlands.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(4,4,11)\n",
    "plt.xlabel('Pakistan')         \n",
    "gdp_df.Pakistan.plot(grid=True, label=\"GDP\", legend=True)\n",
    "noedu_table.Pakistan.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(4,4,12)\n",
    "plt.xlabel('Peru')         \n",
    "gdp_df.Peru.plot(grid=True, label=\"GDP\", legend=True)\n",
    "noedu_table.Peru.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(4,4,13)\n",
    "plt.xlabel('Spain')         \n",
    "gdp_df.Spain.plot(grid=True, label=\"GDP\", legend=True)\n",
    "noedu_table.Spain.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(4,4,14)\n",
    "plt.xlabel('Switzerland')         \n",
    "gdp_df.Switzerland.plot(grid=True, label=\"GDP\", legend=True)\n",
    "noedu_table.Switzerland.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(4,4,15)\n",
    "plt.xlabel('Thailand')         \n",
    "gdp_df.Thailand.plot(grid=True, label=\"GDP\", legend=True)\n",
    "noedu_table.Thailand.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.subplot(4,4,16)\n",
    "plt.xlabel('United States')         \n",
    "gdp_df['United States'].plot(grid=True, label=\"GDP\", legend=True)\n",
    "noedu_table['United States'].plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combination 2 (No Environmental Conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cat_yearly' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-47f44993cb47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnoenvi_table\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcat_yearly\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'2000'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2001'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2002'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2003'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2004'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2005'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2006'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2007'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2008'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2009'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2010'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2011'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2012'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2013'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2014'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2015'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cat_yearly' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "noenvi_table=pd.DataFrame(index=cat_yearly[0][0].index,columns=['2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015'])\n",
    "\n",
    "\n",
    "\n",
    "for x in range(0,16):   \n",
    "  for index, row in yearly_noenvi[x].iterrows():\n",
    "          if x==0:\n",
    "              mean=st.mean(row)\n",
    "              noenvi_table.loc[noenvi_table.index==row.name,'2000']=mean\n",
    "          elif x==1:\n",
    "              mean=st.mean(row)\n",
    "              noenvi_table.loc[noenvi_table.index==row.name,'2001']=mean\n",
    "          elif x==2:\n",
    "              mean=st.mean(row)\n",
    "              noenvi_table.loc[noenvi_table.index==row.name,'2002']=mean\n",
    "          elif x==3:\n",
    "              mean=st.mean(row)\n",
    "              noenvi_table.loc[noenvi_table.index==row.name,'2003']=mean\n",
    "          elif x==4:\n",
    "              mean=st.mean(row)\n",
    "              noenvi_table.loc[noenvi_table.index==row.name,'2004']=mean\n",
    "          elif x==5:\n",
    "              mean=st.mean(row)\n",
    "              noenvi_table.loc[noenvi_table.index==row.name,'2005']=mean\n",
    "          elif x==6:\n",
    "              mean=st.mean(row)\n",
    "              noenvi_table.loc[noenvi_table.index==row.name,'2006']=mean\n",
    "          elif x==7:\n",
    "              mean=st.mean(row)\n",
    "              noenvi_table.loc[noenvi_table.index==row.name,'2007']=mean\n",
    "          elif x==8:\n",
    "              mean=st.mean(row)\n",
    "              noenvi_table.loc[noenvi_table.index==row.name,'2008']=mean\n",
    "          elif x==9:\n",
    "              mean=st.mean(row)\n",
    "              noenvi_table.loc[noenvi_table.index==row.name,'2009']=mean\n",
    "          elif x==10:\n",
    "              mean=st.mean(row)\n",
    "              noenvi_table.loc[noenvi_table.index==row.name,'2010']=mean\n",
    "          elif x==11:\n",
    "              mean=st.mean(row)\n",
    "              noenvi_table.loc[noenvi_table.index==row.name,'2011']=mean\n",
    "          elif x==12:\n",
    "              mean=st.mean(row)\n",
    "              noenvi_table.loc[noenvi_table.index==row.name,'2012']=mean\n",
    "          elif x==13:\n",
    "              mean=st.mean(row)\n",
    "              noenvi_table.loc[noenvi_table.index==row.name,'2013']=mean\n",
    "          elif x==14:\n",
    "              mean=st.mean(row)\n",
    "              noenvi_table.loc[noenvi_table.index==row.name,'2014']=mean              \n",
    "          else:\n",
    "              mean=st.mean(row)\n",
    "              noenvi_table.loc[noenvi_table.index==row.name,'2015']=mean\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "noenvi_table=noenvi_table.T           \n",
    "\n",
    "\n",
    "                \n",
    "            \n",
    "noenvi_table = noenvi_table.set_index(pd.to_datetime(noenvi_table.index))            \n",
    "\n",
    "    \n",
    "    \n",
    "fig, axs = plt.subplots(4, 4)\n",
    "fig.tight_layout()\n",
    "\n",
    "\n",
    "plt.subplot(441)\n",
    "plt.xlabel('Argentina')         \n",
    "gdp_df.Argentina.plot(grid=True, label=\"GDP\", legend=True)\n",
    "noenvi_table.Argentina.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(442)\n",
    "plt.xlabel('Brazil')         \n",
    "gdp_df.Brazil.plot(grid=True, label=\"GDP\", legend=True)\n",
    "noenvi_table.Brazil.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(443)\n",
    "plt.xlabel('Chile')         \n",
    "gdp_df.Chile.plot(grid=True, label=\"GDP\", legend=True)\n",
    "noenvi_table.Chile.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(444)\n",
    "plt.xlabel('France')         \n",
    "gdp_df.France.plot(grid=True, label=\"GDP\", legend=True)\n",
    "noenvi_table.France.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(445)\n",
    "plt.xlabel('Ireland')         \n",
    "gdp_df.Ireland.plot(grid=True, label=\"GDP\", legend=True)\n",
    "noenvi_table.Ireland.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(446)\n",
    "plt.xlabel('Italy')         \n",
    "gdp_df.Italy.plot(grid=True, label=\"GDP\", legend=True)\n",
    "noenvi_table.Italy.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(447)\n",
    "plt.xlabel('Latvia')         \n",
    "gdp_df.Latvia.plot(grid=True, label=\"GDP\", legend=True)\n",
    "noenvi_table.Latvia.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(448)\n",
    "plt.xlabel('Lithuania')         \n",
    "gdp_df.Lithuania.plot(grid=True, label=\"GDP\", legend=True)\n",
    "noenvi_table.Lithuania.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(4,4,9)\n",
    "plt.xlabel('Malaysia')         \n",
    "gdp_df.Malaysia.plot(grid=True, label=\"GDP\", legend=True)\n",
    "noenvi_table.Malaysia.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(4,4,10)\n",
    "plt.xlabel('Netherlands')         \n",
    "gdp_df.Netherlands.plot(grid=True, label=\"GDP\", legend=True)\n",
    "noenvi_table.Netherlands.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(4,4,11)\n",
    "plt.xlabel('Pakistan')         \n",
    "gdp_df.Pakistan.plot(grid=True, label=\"GDP\", legend=True)\n",
    "noenvi_table.Pakistan.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(4,4,12)\n",
    "plt.xlabel('Peru')         \n",
    "gdp_df.Peru.plot(grid=True, label=\"GDP\", legend=True)\n",
    "noenvi_table.Peru.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(4,4,13)\n",
    "plt.xlabel('Spain')         \n",
    "gdp_df.Spain.plot(grid=True, label=\"GDP\", legend=True)\n",
    "noenvi_table.Spain.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(4,4,14)\n",
    "plt.xlabel('Switzerland')         \n",
    "gdp_df.Switzerland.plot(grid=True, label=\"GDP\", legend=True)\n",
    "noenvi_table.Switzerland.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(4,4,15)\n",
    "plt.xlabel('Thailand')         \n",
    "gdp_df.Thailand.plot(grid=True, label=\"GDP\", legend=True)\n",
    "noenvi_table.Thailand.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.subplot(4,4,16)\n",
    "plt.xlabel('United States')         \n",
    "gdp_df['United States'].plot(grid=True, label=\"GDP\", legend=True)\n",
    "noenvi_table['United States'].plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combination 3 (No Health Conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "nohealth_table=pd.DataFrame(index=cat_yearly[0][0].index,columns=['2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015'])\n",
    "\n",
    "\n",
    "\n",
    "for x in range(0,16):   \n",
    "  for index, row in yearly_nohealth[x].iterrows():\n",
    "          if x==0:\n",
    "              mean=st.mean(row)\n",
    "              nohealth_table.loc[nohealth_table.index==row.name,'2000']=mean\n",
    "          elif x==1:\n",
    "              mean=st.mean(row)\n",
    "              nohealth_table.loc[nohealth_table.index==row.name,'2001']=mean\n",
    "          elif x==2:\n",
    "              mean=st.mean(row)\n",
    "              nohealth_table.loc[nohealth_table.index==row.name,'2002']=mean\n",
    "          elif x==3:\n",
    "              mean=st.mean(row)\n",
    "              nohealth_table.loc[nohealth_table.index==row.name,'2003']=mean\n",
    "          elif x==4:\n",
    "              mean=st.mean(row)\n",
    "              nohealth_table.loc[nohealth_table.index==row.name,'2004']=mean\n",
    "          elif x==5:\n",
    "              mean=st.mean(row)\n",
    "              nohealth_table.loc[nohealth_table.index==row.name,'2005']=mean\n",
    "          elif x==6:\n",
    "              mean=st.mean(row)\n",
    "              nohealth_table.loc[nohealth_table.index==row.name,'2006']=mean\n",
    "          elif x==7:\n",
    "              mean=st.mean(row)\n",
    "              nohealth_table.loc[nohealth_table.index==row.name,'2007']=mean\n",
    "          elif x==8:\n",
    "              mean=st.mean(row)\n",
    "              nohealth_table.loc[nohealth_table.index==row.name,'2008']=mean\n",
    "          elif x==9:\n",
    "              mean=st.mean(row)\n",
    "              nohealth_table.loc[nohealth_table.index==row.name,'2009']=mean\n",
    "          elif x==10:\n",
    "              mean=st.mean(row)\n",
    "              nohealth_table.loc[nohealth_table.index==row.name,'2010']=mean\n",
    "          elif x==11:\n",
    "              mean=st.mean(row)\n",
    "              nohealth_table.loc[nohealth_table.index==row.name,'2011']=mean\n",
    "          elif x==12:\n",
    "              mean=st.mean(row)\n",
    "              nohealth_table.loc[nohealth_table.index==row.name,'2012']=mean\n",
    "          elif x==13:\n",
    "              mean=st.mean(row)\n",
    "              nohealth_table.loc[nohealth_table.index==row.name,'2013']=mean\n",
    "          elif x==14:\n",
    "              mean=st.mean(row)\n",
    "              nohealth_table.loc[nohealth_table.index==row.name,'2014']=mean              \n",
    "          else:\n",
    "              mean=st.mean(row)\n",
    "              nohealth_table.loc[nohealth_table.index==row.name,'2015']=mean\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "nohealth_table=nohealth_table.T           \n",
    "\n",
    "\n",
    "                \n",
    "            \n",
    "nohealth_table = nohealth_table.set_index(pd.to_datetime(nohealth_table.index))            \n",
    "\n",
    "    \n",
    "    \n",
    "fig, axs = plt.subplots(4, 4)\n",
    "fig.tight_layout()\n",
    "\n",
    "\n",
    "plt.subplot(441)\n",
    "plt.xlabel('Argentina')         \n",
    "gdp_df.Argentina.plot(grid=True, label=\"GDP\", legend=True)\n",
    "nohealth_table.Argentina.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(442)\n",
    "plt.xlabel('Brazil')         \n",
    "gdp_df.Brazil.plot(grid=True, label=\"GDP\", legend=True)\n",
    "nohealth_table.Brazil.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(443)\n",
    "plt.xlabel('Chile')         \n",
    "gdp_df.Chile.plot(grid=True, label=\"GDP\", legend=True)\n",
    "nohealth_table.Chile.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(444)\n",
    "plt.xlabel('France')         \n",
    "gdp_df.France.plot(grid=True, label=\"GDP\", legend=True)\n",
    "nohealth_table.France.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(445)\n",
    "plt.xlabel('Ireland')         \n",
    "gdp_df.Ireland.plot(grid=True, label=\"GDP\", legend=True)\n",
    "nohealth_table.Ireland.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(446)\n",
    "plt.xlabel('Italy')         \n",
    "gdp_df.Italy.plot(grid=True, label=\"GDP\", legend=True)\n",
    "nohealth_table.Italy.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(447)\n",
    "plt.xlabel('Latvia')         \n",
    "gdp_df.Latvia.plot(grid=True, label=\"GDP\", legend=True)\n",
    "nohealth_table.Latvia.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(448)\n",
    "plt.xlabel('Lithuania')         \n",
    "gdp_df.Lithuania.plot(grid=True, label=\"GDP\", legend=True)\n",
    "nohealth_table.Lithuania.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(4,4,9)\n",
    "plt.xlabel('Malaysia')         \n",
    "gdp_df.Malaysia.plot(grid=True, label=\"GDP\", legend=True)\n",
    "nohealth_table.Malaysia.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(4,4,10)\n",
    "plt.xlabel('Netherlands')         \n",
    "gdp_df.Netherlands.plot(grid=True, label=\"GDP\", legend=True)\n",
    "nohealth_table.Netherlands.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(4,4,11)\n",
    "plt.xlabel('Pakistan')         \n",
    "gdp_df.Pakistan.plot(grid=True, label=\"GDP\", legend=True)\n",
    "nohealth_table.Pakistan.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(4,4,12)\n",
    "plt.xlabel('Peru')         \n",
    "gdp_df.Peru.plot(grid=True, label=\"GDP\", legend=True)\n",
    "nohealth_table.Peru.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(4,4,13)\n",
    "plt.xlabel('Spain')         \n",
    "gdp_df.Spain.plot(grid=True, label=\"GDP\", legend=True)\n",
    "nohealth_table.Spain.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(4,4,14)\n",
    "plt.xlabel('Switzerland')         \n",
    "gdp_df.Switzerland.plot(grid=True, label=\"GDP\", legend=True)\n",
    "nohealth_table.Switzerland.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(4,4,15)\n",
    "plt.xlabel('Thailand')         \n",
    "gdp_df.Thailand.plot(grid=True, label=\"GDP\", legend=True)\n",
    "nohealth_table.Thailand.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.subplot(4,4,16)\n",
    "plt.xlabel('United States')         \n",
    "gdp_df['United States'].plot(grid=True, label=\"GDP\", legend=True)\n",
    "nohealth_table['United States'].plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combination 4 (No Infrastructure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "noinf_table=pd.DataFrame(index=cat_yearly[0][0].index,columns=['2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015'])\n",
    "\n",
    "\n",
    "\n",
    "for x in range(0,16):   \n",
    "  for index, row in yearly_noinf[x].iterrows():\n",
    "          if x==0:\n",
    "              mean=st.mean(row)\n",
    "              noinf_table.loc[noinf_table.index==row.name,'2000']=mean\n",
    "          elif x==1:\n",
    "              mean=st.mean(row)\n",
    "              noinf_table.loc[noinf_table.index==row.name,'2001']=mean\n",
    "          elif x==2:\n",
    "              mean=st.mean(row)\n",
    "              noinf_table.loc[noinf_table.index==row.name,'2002']=mean\n",
    "          elif x==3:\n",
    "              mean=st.mean(row)\n",
    "              noinf_table.loc[noinf_table.index==row.name,'2003']=mean\n",
    "          elif x==4:\n",
    "              mean=st.mean(row)\n",
    "              noinf_table.loc[noinf_table.index==row.name,'2004']=mean\n",
    "          elif x==5:\n",
    "              mean=st.mean(row)\n",
    "              noinf_table.loc[noinf_table.index==row.name,'2005']=mean\n",
    "          elif x==6:\n",
    "              mean=st.mean(row)\n",
    "              noinf_table.loc[noinf_table.index==row.name,'2006']=mean\n",
    "          elif x==7:\n",
    "              mean=st.mean(row)\n",
    "              noinf_table.loc[noinf_table.index==row.name,'2007']=mean\n",
    "          elif x==8:\n",
    "              mean=st.mean(row)\n",
    "              noinf_table.loc[noinf_table.index==row.name,'2008']=mean\n",
    "          elif x==9:\n",
    "              mean=st.mean(row)\n",
    "              noinf_table.loc[noinf_table.index==row.name,'2009']=mean\n",
    "          elif x==10:\n",
    "              mean=st.mean(row)\n",
    "              noinf_table.loc[noinf_table.index==row.name,'2010']=mean\n",
    "          elif x==11:\n",
    "              mean=st.mean(row)\n",
    "              noinf_table.loc[noinf_table.index==row.name,'2011']=mean\n",
    "          elif x==12:\n",
    "              mean=st.mean(row)\n",
    "              noinf_table.loc[noinf_table.index==row.name,'2012']=mean\n",
    "          elif x==13:\n",
    "              mean=st.mean(row)\n",
    "              noinf_table.loc[noinf_table.index==row.name,'2013']=mean\n",
    "          elif x==14:\n",
    "              mean=st.mean(row)\n",
    "              noinf_table.loc[noinf_table.index==row.name,'2014']=mean              \n",
    "          else:\n",
    "              mean=st.mean(row)\n",
    "              noinf_table.loc[noinf_table.index==row.name,'2015']=mean\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "noinf_table=noinf_table.T           \n",
    "\n",
    "\n",
    "                \n",
    "            \n",
    "noinf_table = noinf_table.set_index(pd.to_datetime(noinf_table.index))            \n",
    "\n",
    "    \n",
    "    \n",
    "fig, axs = plt.subplots(4, 4)\n",
    "fig.tight_layout()\n",
    "\n",
    "\n",
    "plt.subplot(441)\n",
    "plt.xlabel('Argentina')         \n",
    "gdp_df.Argentina.plot(grid=True, label=\"GDP\", legend=True)\n",
    "noinf_table.Argentina.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(442)\n",
    "plt.xlabel('Brazil')         \n",
    "gdp_df.Brazil.plot(grid=True, label=\"GDP\", legend=True)\n",
    "noinf_table.Brazil.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(443)\n",
    "plt.xlabel('Chile')         \n",
    "gdp_df.Chile.plot(grid=True, label=\"GDP\", legend=True)\n",
    "noinf_table.Chile.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(444)\n",
    "plt.xlabel('France')         \n",
    "gdp_df.France.plot(grid=True, label=\"GDP\", legend=True)\n",
    "noinf_table.France.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(445)\n",
    "plt.xlabel('Ireland')         \n",
    "gdp_df.Ireland.plot(grid=True, label=\"GDP\", legend=True)\n",
    "noinf_table.Ireland.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(446)\n",
    "plt.xlabel('Italy')         \n",
    "gdp_df.Italy.plot(grid=True, label=\"GDP\", legend=True)\n",
    "noinf_table.Italy.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(447)\n",
    "plt.xlabel('Latvia')         \n",
    "gdp_df.Latvia.plot(grid=True, label=\"GDP\", legend=True)\n",
    "noinf_table.Latvia.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(448)\n",
    "plt.xlabel('Lithuania')         \n",
    "gdp_df.Lithuania.plot(grid=True, label=\"GDP\", legend=True)\n",
    "noinf_table.Lithuania.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(4,4,9)\n",
    "plt.xlabel('Malaysia')         \n",
    "gdp_df.Malaysia.plot(grid=True, label=\"GDP\", legend=True)\n",
    "noinf_table.Malaysia.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(4,4,10)\n",
    "plt.xlabel('Netherlands')         \n",
    "gdp_df.Netherlands.plot(grid=True, label=\"GDP\", legend=True)\n",
    "noinf_table.Netherlands.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(4,4,11)\n",
    "plt.xlabel('Pakistan')         \n",
    "gdp_df.Pakistan.plot(grid=True, label=\"GDP\", legend=True)\n",
    "noinf_table.Pakistan.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(4,4,12)\n",
    "plt.xlabel('Peru')         \n",
    "gdp_df.Peru.plot(grid=True, label=\"GDP\", legend=True)\n",
    "noinf_table.Peru.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(4,4,13)\n",
    "plt.xlabel('Spain')         \n",
    "gdp_df.Spain.plot(grid=True, label=\"GDP\", legend=True)\n",
    "noinf_table.Spain.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(4,4,14)\n",
    "plt.xlabel('Switzerland')         \n",
    "gdp_df.Switzerland.plot(grid=True, label=\"GDP\", legend=True)\n",
    "noinf_table.Switzerland.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(4,4,15)\n",
    "plt.xlabel('Thailand')         \n",
    "gdp_df.Thailand.plot(grid=True, label=\"GDP\", legend=True)\n",
    "noinf_table.Thailand.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.subplot(4,4,16)\n",
    "plt.xlabel('United States')         \n",
    "gdp_df['United States'].plot(grid=True, label=\"GDP\", legend=True)\n",
    "noinf_table['United States'].plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combination 5 (No Labor Conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "nolab_table=pd.DataFrame(index=cat_yearly[0][0].index,columns=['2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015'])\n",
    "\n",
    "\n",
    "\n",
    "for x in range(0,16):   \n",
    "  for index, row in yearly_nolab[x].iterrows():\n",
    "          if x==0:\n",
    "              mean=st.mean(row)\n",
    "              nolab_table.loc[nolab_table.index==row.name,'2000']=mean\n",
    "          elif x==1:\n",
    "              mean=st.mean(row)\n",
    "              nolab_table.loc[nolab_table.index==row.name,'2001']=mean\n",
    "          elif x==2:\n",
    "              mean=st.mean(row)\n",
    "              nolab_table.loc[nolab_table.index==row.name,'2002']=mean\n",
    "          elif x==3:\n",
    "              mean=st.mean(row)\n",
    "              nolab_table.loc[nolab_table.index==row.name,'2003']=mean\n",
    "          elif x==4:\n",
    "              mean=st.mean(row)\n",
    "              nolab_table.loc[nolab_table.index==row.name,'2004']=mean\n",
    "          elif x==5:\n",
    "              mean=st.mean(row)\n",
    "              nolab_table.loc[nolab_table.index==row.name,'2005']=mean\n",
    "          elif x==6:\n",
    "              mean=st.mean(row)\n",
    "              nolab_table.loc[nolab_table.index==row.name,'2006']=mean\n",
    "          elif x==7:\n",
    "              mean=st.mean(row)\n",
    "              nolab_table.loc[nolab_table.index==row.name,'2007']=mean\n",
    "          elif x==8:\n",
    "              mean=st.mean(row)\n",
    "              nolab_table.loc[nolab_table.index==row.name,'2008']=mean\n",
    "          elif x==9:\n",
    "              mean=st.mean(row)\n",
    "              nolab_table.loc[nolab_table.index==row.name,'2009']=mean\n",
    "          elif x==10:\n",
    "              mean=st.mean(row)\n",
    "              nolab_table.loc[nolab_table.index==row.name,'2010']=mean\n",
    "          elif x==11:\n",
    "              mean=st.mean(row)\n",
    "              nolab_table.loc[nolab_table.index==row.name,'2011']=mean\n",
    "          elif x==12:\n",
    "              mean=st.mean(row)\n",
    "              nolab_table.loc[nolab_table.index==row.name,'2012']=mean\n",
    "          elif x==13:\n",
    "              mean=st.mean(row)\n",
    "              nolab_table.loc[nolab_table.index==row.name,'2013']=mean\n",
    "          elif x==14:\n",
    "              mean=st.mean(row)\n",
    "              nolab_table.loc[nolab_table.index==row.name,'2014']=mean              \n",
    "          else:\n",
    "              mean=st.mean(row)\n",
    "              nolab_table.loc[nolab_table.index==row.name,'2015']=mean\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "nolab_table=nolab_table.T           \n",
    "\n",
    "\n",
    "                \n",
    "            \n",
    "nolab_table = nolab_table.set_index(pd.to_datetime(nolab_table.index))            \n",
    "\n",
    "    \n",
    "    \n",
    "fig, axs = plt.subplots(4, 4)\n",
    "fig.tight_layout()\n",
    "\n",
    "\n",
    "plt.subplot(441)\n",
    "plt.xlabel('Argentina')         \n",
    "gdp_df.Argentina.plot(grid=True, label=\"GDP\", legend=True)\n",
    "nolab_table.Argentina.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(442)\n",
    "plt.xlabel('Brazil')         \n",
    "gdp_df.Brazil.plot(grid=True, label=\"GDP\", legend=True)\n",
    "nolab_table.Brazil.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(443)\n",
    "plt.xlabel('Chile')         \n",
    "gdp_df.Chile.plot(grid=True, label=\"GDP\", legend=True)\n",
    "nolab_table.Chile.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(444)\n",
    "plt.xlabel('France')         \n",
    "gdp_df.France.plot(grid=True, label=\"GDP\", legend=True)\n",
    "nolab_table.France.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(445)\n",
    "plt.xlabel('Ireland')         \n",
    "gdp_df.Ireland.plot(grid=True, label=\"GDP\", legend=True)\n",
    "nolab_table.Ireland.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(446)\n",
    "plt.xlabel('Italy')         \n",
    "gdp_df.Italy.plot(grid=True, label=\"GDP\", legend=True)\n",
    "nolab_table.Italy.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(447)\n",
    "plt.xlabel('Latvia')         \n",
    "gdp_df.Latvia.plot(grid=True, label=\"GDP\", legend=True)\n",
    "nolab_table.Latvia.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(448)\n",
    "plt.xlabel('Lithuania')         \n",
    "gdp_df.Lithuania.plot(grid=True, label=\"GDP\", legend=True)\n",
    "nolab_table.Lithuania.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(4,4,9)\n",
    "plt.xlabel('Malaysia')         \n",
    "gdp_df.Malaysia.plot(grid=True, label=\"GDP\", legend=True)\n",
    "nolab_table.Malaysia.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(4,4,10)\n",
    "plt.xlabel('Netherlands')         \n",
    "gdp_df.Netherlands.plot(grid=True, label=\"GDP\", legend=True)\n",
    "nolab_table.Netherlands.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(4,4,11)\n",
    "plt.xlabel('Pakistan')         \n",
    "gdp_df.Pakistan.plot(grid=True, label=\"GDP\", legend=True)\n",
    "nolab_table.Pakistan.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(4,4,12)\n",
    "plt.xlabel('Peru')         \n",
    "gdp_df.Peru.plot(grid=True, label=\"GDP\", legend=True)\n",
    "nolab_table.Peru.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(4,4,13)\n",
    "plt.xlabel('Spain')         \n",
    "gdp_df.Spain.plot(grid=True, label=\"GDP\", legend=True)\n",
    "nolab_table.Spain.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(4,4,14)\n",
    "plt.xlabel('Switzerland')         \n",
    "gdp_df.Switzerland.plot(grid=True, label=\"GDP\", legend=True)\n",
    "nolab_table.Switzerland.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(4,4,15)\n",
    "plt.xlabel('Thailand')         \n",
    "gdp_df.Thailand.plot(grid=True, label=\"GDP\", legend=True)\n",
    "nolab_table.Thailand.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.subplot(4,4,16)\n",
    "plt.xlabel('United States')         \n",
    "gdp_df['United States'].plot(grid=True, label=\"GDP\", legend=True)\n",
    "nolab_table['United States'].plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combination 6 (No Social Conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nosoc_table=pd.DataFrame(index=cat_yearly[0][0].index,columns=['2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015'])\n",
    "\n",
    "\n",
    "\n",
    "for x in range(0,16):   \n",
    "  for index, row in yearly_nosoc[x].iterrows():\n",
    "          if x==0:\n",
    "              mean=st.mean(row)\n",
    "              nosoc_table.loc[nosoc_table.index==row.name,'2000']=mean\n",
    "          elif x==1:\n",
    "              mean=st.mean(row)\n",
    "              nosoc_table.loc[nosoc_table.index==row.name,'2001']=mean\n",
    "          elif x==2:\n",
    "              mean=st.mean(row)\n",
    "              nosoc_table.loc[nosoc_table.index==row.name,'2002']=mean\n",
    "          elif x==3:\n",
    "              mean=st.mean(row)\n",
    "              nosoc_table.loc[nosoc_table.index==row.name,'2003']=mean\n",
    "          elif x==4:\n",
    "              mean=st.mean(row)\n",
    "              nosoc_table.loc[nosoc_table.index==row.name,'2004']=mean\n",
    "          elif x==5:\n",
    "              mean=st.mean(row)\n",
    "              nosoc_table.loc[nosoc_table.index==row.name,'2005']=mean\n",
    "          elif x==6:\n",
    "              mean=st.mean(row)\n",
    "              nosoc_table.loc[nosoc_table.index==row.name,'2006']=mean\n",
    "          elif x==7:\n",
    "              mean=st.mean(row)\n",
    "              nosoc_table.loc[nosoc_table.index==row.name,'2007']=mean\n",
    "          elif x==8:\n",
    "              mean=st.mean(row)\n",
    "              nosoc_table.loc[nosoc_table.index==row.name,'2008']=mean\n",
    "          elif x==9:\n",
    "              mean=st.mean(row)\n",
    "              nosoc_table.loc[nosoc_table.index==row.name,'2009']=mean\n",
    "          elif x==10:\n",
    "              mean=st.mean(row)\n",
    "              nosoc_table.loc[nosoc_table.index==row.name,'2010']=mean\n",
    "          elif x==11:\n",
    "              mean=st.mean(row)\n",
    "              nosoc_table.loc[nosoc_table.index==row.name,'2011']=mean\n",
    "          elif x==12:\n",
    "              mean=st.mean(row)\n",
    "              nosoc_table.loc[nosoc_table.index==row.name,'2012']=mean\n",
    "          elif x==13:\n",
    "              mean=st.mean(row)\n",
    "              nosoc_table.loc[nosoc_table.index==row.name,'2013']=mean\n",
    "          elif x==14:\n",
    "              mean=st.mean(row)\n",
    "              nosoc_table.loc[nosoc_table.index==row.name,'2014']=mean              \n",
    "          else:\n",
    "              mean=st.mean(row)\n",
    "              nosoc_table.loc[nosoc_table.index==row.name,'2015']=mean\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "nosoc_table=nosoc_table.T           \n",
    "\n",
    "\n",
    "                \n",
    "            \n",
    "nosoc_table = nosoc_table.set_index(pd.to_datetime(nosoc_table.index))            \n",
    "\n",
    "    \n",
    "    \n",
    "fig, axs = plt.subplots(4, 4)\n",
    "fig.tight_layout()\n",
    "\n",
    "\n",
    "plt.subplot(441)\n",
    "plt.xlabel('Argentina')         \n",
    "gdp_df.Argentina.plot(grid=True, label=\"GDP\", legend=True)\n",
    "nosoc_table.Argentina.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(442)\n",
    "plt.xlabel('Brazil')         \n",
    "gdp_df.Brazil.plot(grid=True, label=\"GDP\", legend=True)\n",
    "nosoc_table.Brazil.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(443)\n",
    "plt.xlabel('Chile')         \n",
    "gdp_df.Chile.plot(grid=True, label=\"GDP\", legend=True)\n",
    "nosoc_table.Chile.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(444)\n",
    "plt.xlabel('France')         \n",
    "gdp_df.France.plot(grid=True, label=\"GDP\", legend=True)\n",
    "nosoc_table.France.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(445)\n",
    "plt.xlabel('Ireland')         \n",
    "gdp_df.Ireland.plot(grid=True, label=\"GDP\", legend=True)\n",
    "nosoc_table.Ireland.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(446)\n",
    "plt.xlabel('Italy')         \n",
    "gdp_df.Italy.plot(grid=True, label=\"GDP\", legend=True)\n",
    "nosoc_table.Italy.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(447)\n",
    "plt.xlabel('Latvia')         \n",
    "gdp_df.Latvia.plot(grid=True, label=\"GDP\", legend=True)\n",
    "nosoc_table.Latvia.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(448)\n",
    "plt.xlabel('Lithuania')         \n",
    "gdp_df.Lithuania.plot(grid=True, label=\"GDP\", legend=True)\n",
    "nosoc_table.Lithuania.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(4,4,9)\n",
    "plt.xlabel('Malaysia')         \n",
    "gdp_df.Malaysia.plot(grid=True, label=\"GDP\", legend=True)\n",
    "nosoc_table.Malaysia.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(4,4,10)\n",
    "plt.xlabel('Netherlands')         \n",
    "gdp_df.Netherlands.plot(grid=True, label=\"GDP\", legend=True)\n",
    "nosoc_table.Netherlands.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(4,4,11)\n",
    "plt.xlabel('Pakistan')         \n",
    "gdp_df.Pakistan.plot(grid=True, label=\"GDP\", legend=True)\n",
    "nosoc_table.Pakistan.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(4,4,12)\n",
    "plt.xlabel('Peru')         \n",
    "gdp_df.Peru.plot(grid=True, label=\"GDP\", legend=True)\n",
    "nosoc_table.Peru.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(4,4,13)\n",
    "plt.xlabel('Spain')         \n",
    "gdp_df.Spain.plot(grid=True, label=\"GDP\", legend=True)\n",
    "nosoc_table.Spain.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(4,4,14)\n",
    "plt.xlabel('Switzerland')         \n",
    "gdp_df.Switzerland.plot(grid=True, label=\"GDP\", legend=True)\n",
    "nosoc_table.Switzerland.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.subplot(4,4,15)\n",
    "plt.xlabel('Thailand')         \n",
    "gdp_df.Thailand.plot(grid=True, label=\"GDP\", legend=True)\n",
    "nosoc_table.Thailand.plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.subplot(4,4,16)\n",
    "plt.xlabel('United States')         \n",
    "gdp_df['United States'].plot(grid=True, label=\"GDP\", legend=True)\n",
    "nosoc_table['United States'].plot(secondary_y=True, label=\"Wellbeing Index\", legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion and conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "52e1cf5783dd1971d265a8ea172e297a",
     "grade": true,
     "grade_id": "cell-4b5c9f698b295ad7",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "A growing number of researchers in an interdisciplinary social sciences framework have proved and believe that the marginal benefits of a further increase in GDP when a certain value threshold has been based, are not significant, and thus, the economically developed countries should focus their attention on phenomena of social pathologies, instead of growth.\n",
    "\n",
    "What the present analysis tried to investigate was based on the claim above, but taken one step further, hypothesis-wise. That is the implied impact of not diverting the focus to these social pathologies when you have passed a given wealth threshold. Thus, the goal was to contribute to proving the importance of maintaining healthy institutions, and most importantly, the trust of the general public towards them.\n",
    "\n",
    "Unfortunately, based on the statistical methods and measures employed, the Wellbeing and Development Construct, in this case, is not credible enough to be considered a useful tool in a policy-maker toolbox. What may have gone wrong, starts from the unavailability of data, and thus, of variables across the countries, leading to a hardly representative sample and low dimensionality. The above condition resulted in a low number of variables used in the majority of the manually constructed categories, which probably were not enough to represent their quality as a compounded factor. Also, the manual construction itself may have proved problematic, as there may have been an overlapping of qualities among several variables, and their predominant one may have been different than the one hypothesized. Moreover, the qualities expected could have also been different, and a more advanced classification algorithm could have been used. Finally, there could have been other types of relations among the variables used across the categories, concerning their group effect (i.e., multiplicative instead of additive).\n",
    "\n",
    "But let us interpret the results as they came up. Firstly, it should be noted that the aggregate levels of the average values of the major categories of Education, Environment, Health, Labor, and Social Conditions across those countries vary greatly. That means, the observed decrease in high-income countries is being actualized in a relatively high level of the index (e.g., in the Netherlands, the index ranges from 80 to 90, and the decrease takes place inside this high range), while in the low-income countries, an increase takes place in a relatively low level (e.g., in Thailand an increase can be spotted, but in the low range of 25 to 35). Theoretically, it should be expected that a decrease in a high range country may not present a threat to social cohesion, but for the low range countries, it may have detrimental effects. Also, the gains, if they were to take place, are decreasing as time goes, and they are characterized by a range of maximum values, that is, not occurring indefinitely.\n",
    "\n",
    "In conclusion, based on the literal results of the analysis, it can be drawn that the GDP may be incapable of embodying the complexity of societies and oversimplifies their ontology to an easily computable measure, that acts as a mere heuristic and inadequate proxy of societal health and efficiency. That of course does not imply that GDP is not a good approximation of the cohesion/institutional state of a given society."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
